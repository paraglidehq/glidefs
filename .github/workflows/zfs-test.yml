name: ZFS on ZeroFS Test

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  zfs-test:
    name: Run ZFS on top of ZeroFS
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Start MinIO
        run: |
          docker run -d \
            --name minio \
            -p 9000:9000 \
            -e MINIO_ROOT_USER=minioadmin \
            -e MINIO_ROOT_PASSWORD=minioadmin \
            minio/minio server /data

          # Wait for MinIO to be ready
          for i in {1..30}; do
            if curl -f http://localhost:9000/minio/health/live; then
              echo "MinIO is ready"
              break
            fi
            sleep 1
          done

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y nbd-client zfsutils-linux wget pigz pixz parallel

      - name: Setup MinIO bucket
        run: |
          # Install mc (MinIO client)
          wget https://dl.min.io/client/mc/release/linux-amd64/mc
          chmod +x mc
          ./mc alias set myminio http://localhost:9000 minioadmin minioadmin
          # Clear any stale data from previous runs (leases, cache metadata, etc.)
          ./mc rm --recursive --force myminio/zerofs-zfs-test/ || true
          ./mc mb myminio/zerofs-zfs-test || true

      - name: Build ZeroFS
        working-directory: zerofs
        run: cargo build --profile ci

      - name: Start ZeroFS
        working-directory: zerofs
        run: |
          # Create cache directory
          mkdir -p /tmp/zerofs-cache

          # Create ZeroFS configuration
          cat > zerofs-ci.toml << EOF
          [cache]
          dir = "/tmp/zerofs-cache"
          disk_size_gb = 2.0
          memory_size_gb = 2.0

          [storage]
          url = "s3://zerofs-zfs-test/zfs-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "test-device"
          size_gb = 3.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Start ZeroFS with NBD support in the background
          ./target/ci/zerofs run -c zerofs-ci.toml &

          # Wait for ZeroFS NBD server to start
          echo "Waiting for ZeroFS NBD server to start..."
          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then
              echo "ZeroFS NBD server is ready"
              break
            fi
            sleep 1
          done

          # Verify ZeroFS NBD server is running
          if ! nc -z 127.0.0.1 10809; then
            echo "ZeroFS NBD server failed to start"
            exit 1
          fi

      - name: Connect NBD device
        run: |
          echo "Connecting to NBD device..."
          sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N test-device

          sudo blockdev --getsize64 /dev/nbd0
          sudo fdisk -l /dev/nbd0

      - name: Create ZFS pool
        run: |
          # Create ZFS pool directly on NBD block device
          echo "Creating ZFS pool on NBD device..."
          sudo zpool create testpool /dev/nbd0

          # Check pool status
          sudo zpool status testpool
          sudo zpool list testpool

      - name: Create ZFS filesystem
        run: |
          # Create a ZFS filesystem
          sudo zfs create testpool/data

          # Set mountpoint
          sudo zfs set mountpoint=/mnt/zfsdata testpool/data

          # Set copies=2 for redundancy (allows scrub to detect/repair corruption)
          sudo zfs set copies=2 testpool/data

          # List filesystems
          sudo zfs list

      - name: Download and extract Linux kernel
        run: |
          # Download Linux kernel source
          echo "Downloading Linux kernel 6.15.6..."
          cd /mnt/zfsdata
          sudo wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.15.6.tar.xz

          # Show download size
          ls -lh linux-6.15.6.tar.xz

          echo "Extracting kernel source..."
          time sudo tar -I pixz -xf linux-6.15.6.tar.xz

          # Count files to verify extraction
          echo "Counting extracted files..."
          sudo find linux-6.15.6 -type f | wc -l

      - name: Test ZFS operations
        run: |
          # Create a snapshot
          echo "Creating ZFS snapshot..."
          sudo zfs snapshot testpool/data@after-kernel

          # List snapshots
          sudo zfs list -t snapshot

          # Show pool I/O statistics
          sudo zpool iostat testpool 1 5

          # Create some test files
          echo "Creating test files..."
          sudo dd if=/dev/urandom of=/mnt/zfsdata/random.dat bs=1M count=100

          # Create another snapshot
          sudo zfs snapshot testpool/data@after-random

          # Show space usage
          sudo zfs list -o name,used,avail,refer,mountpoint

      - name: Benchmark snapshot and clone latency
        run: |
          echo "=== SNAPSHOT/CLONE LATENCY BENCHMARK ==="
          echo ""
          echo "Comparing write-behind (current) vs write-through (alternative):"
          echo "- Write-behind: flush() syncs to LOCAL SSD only → fast snapshots"
          echo "- Write-through: flush() would sync to S3 → slow snapshots"
          echo ""

          # Benchmark: 10 snapshots (write-behind - what we have)
          echo "--- WRITE-BEHIND (current implementation) ---"
          echo "Creating 10 snapshots with local-only flush..."
          TOTAL_SNAP_MS=0
          for i in {1..10}; do
            START=$(date +%s%3N)
            sudo zfs snapshot testpool/data@bench-snap-$i
            END=$(date +%s%3N)
            ELAPSED=$((END - START))
            echo "  Snapshot $i: ${ELAPSED}ms"
            TOTAL_SNAP_MS=$((TOTAL_SNAP_MS + ELAPSED))
          done
          AVG_SNAP_MS=$((TOTAL_SNAP_MS / 10))
          echo "Average snapshot latency (write-behind): ${AVG_SNAP_MS}ms"

          # Benchmark: 10 clones
          echo ""
          echo "Creating 10 clones..."
          TOTAL_CLONE_MS=0
          for i in {1..10}; do
            START=$(date +%s%3N)
            sudo zfs clone testpool/data@bench-snap-1 testpool/bench-clone-$i
            END=$(date +%s%3N)
            ELAPSED=$((END - START))
            echo "  Clone $i: ${ELAPSED}ms"
            TOTAL_CLONE_MS=$((TOTAL_CLONE_MS + ELAPSED))
          done
          AVG_CLONE_MS=$((TOTAL_CLONE_MS / 10))
          echo "Average clone latency: ${AVG_CLONE_MS}ms"

          # Benchmark: What write-through would cost (drain to S3)
          echo ""
          echo "--- WRITE-THROUGH SIMULATION (what we're avoiding) ---"
          echo "Writing 50MB of dirty data, then measuring drain time..."

          # Write dirty data
          sudo dd if=/dev/urandom of=/mnt/zfsdata/dirty-test.dat bs=1M count=50 2>/dev/null
          sync

          # Trigger drain via SIGUSR1 and measure time
          ZEROFS_PID=$(pgrep -f "zerofs.*run" | head -1)
          if [ -n "$ZEROFS_PID" ]; then
            echo "Triggering S3 drain (this is what write-through would add to each snapshot)..."
            START=$(date +%s%3N)
            kill -USR1 $ZEROFS_PID 2>/dev/null || true
            # Wait for drain to complete (poll dirty block count via metrics or just wait)
            sleep 5  # Give it time to drain 50MB to MinIO
            END=$(date +%s%3N)
            DRAIN_MS=$((END - START))
            echo "S3 drain time for 50MB: ${DRAIN_MS}ms"
            echo ""
            echo "With write-through, EVERY snapshot would pay this penalty!"
          else
            echo "Could not find ZeroFS PID for drain test"
            DRAIN_MS=0
          fi

          # Summary comparison
          echo ""
          echo "=== COMPARISON SUMMARY ==="
          echo "┌─────────────────────────────────────────────────────────────┐"
          echo "│ Operation              │ Write-Behind │ Write-Through*      │"
          echo "├─────────────────────────────────────────────────────────────┤"
          echo "│ Snapshot (avg)         │ ${AVG_SNAP_MS}ms          │ ~${DRAIN_MS}ms+ (est)       │"
          echo "│ Clone (avg)            │ ${AVG_CLONE_MS}ms          │ same                │"
          echo "└─────────────────────────────────────────────────────────────┘"
          echo "* Write-through would add S3 sync latency to every flush/snapshot"
          echo ""

          # Assert: Snapshot must be under 500ms
          echo "=== LATENCY ASSERTIONS ==="
          if [ $AVG_SNAP_MS -lt 500 ]; then
            echo "✓ Snapshot latency OK: ${AVG_SNAP_MS}ms < 500ms"
          else
            echo "✗ Snapshot latency TOO HIGH: ${AVG_SNAP_MS}ms >= 500ms"
            echo "  This suggests flush() is waiting for S3, which is a bug!"
            exit 1
          fi

          if [ $AVG_CLONE_MS -lt 500 ]; then
            echo "✓ Clone latency OK: ${AVG_CLONE_MS}ms < 500ms"
          else
            echo "✗ Clone latency TOO HIGH: ${AVG_CLONE_MS}ms >= 500ms"
            exit 1
          fi

          echo ""
          echo "=== BENCHMARK PASSED ==="
          echo "Write-behind cache provides ${AVG_SNAP_MS}ms snapshots vs ~${DRAIN_MS}ms+ for write-through"
          if [ $DRAIN_MS -gt 0 ] && [ $AVG_SNAP_MS -gt 0 ]; then
            SPEEDUP=$((DRAIN_MS / AVG_SNAP_MS))
            echo "That's approximately ${SPEEDUP}x faster!"
          fi

      - name: Create tarball and checksum for durability test
        run: |
          echo "Creating tarball of ZFS data..."
          cd /mnt/zfsdata
          sudo tar -I pigz -cf /tmp/zfsdata-backup.tar.gz .

          echo "Calculating checksum..."
          sudo sha256sum /tmp/zfsdata-backup.tar.gz > /tmp/zfsdata-backup.sha256
          cat /tmp/zfsdata-backup.sha256

          # Also create checksum of individual files for comparison
          sudo find . -type f -print0 | parallel -0 -j+0 sha256sum | sort > /tmp/file-checksums.txt
          echo "Number of files checksummed: $(wc -l < /tmp/file-checksums.txt)"

      - name: Sync and prepare for durability test
        run: |
          echo "Syncing filesystem..."
          sync
          sudo zpool sync testpool

          echo "Running ZFS TRIM..."
          sudo zpool trim testpool

          # Wait for trim to complete
          echo "Waiting for TRIM to complete..."
          while sudo zpool status testpool | grep -q 'trimming'; do
            sleep 2
            echo "TRIM in progress..."
            sudo zpool status testpool
          done
          echo "TRIM completed"

          echo "Starting ZFS scrub..."
          sudo zpool scrub testpool

          # Wait for scrub to complete
          echo "Waiting for scrub to complete..."
          while sudo zpool status testpool | grep -q 'scrub in progress'; do
            sleep 2
            echo "Scrub in progress..."
            sudo zpool status testpool
          done
          echo "Scrub completed"

          # Show final pool status before restart
          sudo zpool status testpool

      - name: Unmount and export ZFS pool
        run: |
          echo "Unmounting ZFS filesystem..."
          sudo zfs unmount testpool/data

          echo "Exporting ZFS pool..."
          sudo zpool export testpool

          # Verify pool is exported
          if sudo zpool list testpool 2>/dev/null; then
            echo "ERROR: Pool still imported!"
            exit 1
          fi
          echo "Pool successfully exported"

      - name: Disconnect NBD and stop ZeroFS
        run: |
          echo "Disconnecting NBD device..."
          sudo nbd-client -d /dev/nbd0

          # Wait for NBD to fully disconnect - check if device is actually in use
          echo "Waiting for NBD device to disconnect..."
          for i in {1..10}; do
            # Check if the NBD device is in use by looking at /sys/block/nbd0/size
            # When disconnected, this should be 0
            if [ ! -e /sys/block/nbd0/size ] || [ "$(cat /sys/block/nbd0/size 2>/dev/null)" = "0" ]; then
              echo "NBD device disconnected successfully"
              break
            fi
            echo "Waiting for NBD disconnect... attempt $i/10"
            sleep 1
          done

          # Also check using nbd-client -c to see if device is connected
          if sudo nbd-client -c /dev/nbd0 2>/dev/null; then
            echo "WARNING: NBD device reports as still connected, but continuing..."
          else
            echo "NBD device confirmed disconnected"
          fi

          echo "Stopping ZeroFS gracefully..."
          ZEROFS_PID=$(pgrep -f "zerofs.*run.*zerofs-ci.toml" || true)
          if [ -n "$ZEROFS_PID" ]; then
            echo "Found ZeroFS process: $ZEROFS_PID"

            # First, send SIGUSR1 to trigger a full drain to S3
            # This ensures all dirty blocks are flushed before shutdown
            echo "Sending SIGUSR1 to trigger S3 drain..."
            kill -USR1 $ZEROFS_PID || true

            # Wait for the drain to complete (up to 60 seconds for large datasets)
            echo "Waiting for drain to complete..."
            sleep 10

            # Now send SIGTERM for graceful shutdown
            echo "Sending SIGTERM..."
            kill -TERM $ZEROFS_PID || true
          else
            echo "No ZeroFS process found (may have already stopped)"
          fi

          # Wait for ZeroFS to stop (up to 60 seconds - drain can take time)
          echo "Waiting for ZeroFS to stop..."
          for i in {1..60}; do
            if ! pgrep -f "zerofs.*run.*zerofs-ci.toml" > /dev/null; then
              echo "ZeroFS stopped after $i seconds"
              break
            fi
            if [ $((i % 10)) -eq 0 ]; then
              echo "Still waiting for ZeroFS to stop... $i/60 seconds"
            fi
            sleep 1
          done

          # Final check - DO NOT use SIGKILL as it causes data loss
          if pgrep -f "zerofs.*run.*zerofs-ci.toml" > /dev/null; then
            echo "ERROR: ZeroFS did not stop gracefully after 60 seconds!"
            echo "This likely indicates a bug - investigate rather than force kill"
            exit 1
          fi

          # Ensure port is free
          echo "Waiting for port 10809 to be free..."
          for i in {1..10}; do
            if ! nc -z 127.0.0.1 10809 2>/dev/null; then
              echo "Port 10809 is free"
              break
            fi
            echo "Waiting for port to be released... attempt $i/10"
            sleep 1
          done

          # Final check
          if nc -z 127.0.0.1 10809 2>/dev/null; then
            echo "ERROR: Port 10809 still in use after 10 seconds!"
            exit 1
          fi

      - name: Restart ZeroFS
        working-directory: zerofs
        run: |
          echo "Starting ZeroFS again..."
          # Config file should still exist from earlier
          ./target/ci/zerofs run -c zerofs-ci.toml &

          # Wait for ZeroFS NBD server to start
          echo "Waiting for ZeroFS NBD server to restart..."
          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then
              echo "ZeroFS NBD server is ready"
              break
            fi
            sleep 1
          done

          # Verify ZeroFS NBD server is running
          if ! nc -z 127.0.0.1 10809; then
            echo "ZeroFS NBD server failed to restart"
            exit 1
          fi

      - name: Reconnect NBD and import pool
        run: |
          echo "Reconnecting NBD device..."
          sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N test-device

          # Verify NBD device is available
          sudo blockdev --getsize64 /dev/nbd0
          sudo fdisk -l /dev/nbd0

          echo "Importing ZFS pool..."
          sudo zpool import testpool

          # Check pool status
          sudo zpool status testpool
          sudo zfs list

      - name: Verify data integrity
        run: |
          echo "Creating new tarball of restored data..."
          cd /mnt/zfsdata
          sudo tar -I pigz -cf /tmp/zfsdata-restored.tar.gz .

          echo "Calculating checksum of restored data..."
          sudo sha256sum /tmp/zfsdata-restored.tar.gz > /tmp/zfsdata-restored.sha256
          cat /tmp/zfsdata-restored.sha256

          # Compare checksums
          echo "Comparing tarball checksums..."
          ORIGINAL_SUM=$(cut -d' ' -f1 < /tmp/zfsdata-backup.sha256)
          RESTORED_SUM=$(cut -d' ' -f1 < /tmp/zfsdata-restored.sha256)

          if [ "$ORIGINAL_SUM" = "$RESTORED_SUM" ]; then
            echo "SUCCESS: Checksums match! Data integrity verified."
          else
            echo "ERROR: Checksums do not match!"
            echo "Original: $ORIGINAL_SUM"
            echo "Restored: $RESTORED_SUM"
            exit 1
          fi

          # Also verify individual file checksums
          echo "Verifying individual file checksums..."
          sudo find . -type f -print0 | parallel -0 -j+0 sha256sum | sort > /tmp/file-checksums-restored.txt

          if diff /tmp/file-checksums.txt /tmp/file-checksums-restored.txt; then
            echo "SUCCESS: All individual file checksums match!"
          else
            echo "ERROR: Individual file checksums differ!"
            exit 1
          fi

      - name: Show ZeroFS S3 usage and metrics
        run: |
          echo "=== S3 STORAGE USAGE ==="
          ./mc du myminio/zerofs-zfs-test
          echo ""

          echo "=== S3 OBJECT COUNT ==="
          ./mc ls --recursive myminio/zerofs-zfs-test | wc -l
          echo "objects in bucket"
          echo ""

          echo "=== BATCH OBJECT BREAKDOWN ==="
          echo "Data batches (blocks):"
          ./mc ls --recursive myminio/zerofs-zfs-test | grep -c "/batch_" || echo "0"
          echo ""
          echo "Lease objects:"
          ./mc ls --recursive myminio/zerofs-zfs-test | grep -c "/lease_" || echo "0"
          echo ""

          echo "=== WRITE AMPLIFICATION ANALYSIS ==="
          # Calculate what guest wrote vs what's in S3
          GUEST_WRITTEN_MB=$(du -sm /mnt/zfsdata 2>/dev/null | cut -f1 || echo "unknown")
          S3_USAGE=$(./mc du --json myminio/zerofs-zfs-test 2>/dev/null | jq -r '.size // 0' || echo "0")
          S3_USAGE_MB=$((S3_USAGE / 1024 / 1024))

          echo "Guest data on ZFS: ${GUEST_WRITTEN_MB}MB"
          echo "Data in S3: ${S3_USAGE_MB}MB"

          if [ "$GUEST_WRITTEN_MB" != "unknown" ] && [ "$GUEST_WRITTEN_MB" -gt 0 ]; then
            # Calculate write amplification (S3 / guest)
            # Use awk for floating point
            WRITE_AMP=$(echo "$S3_USAGE_MB $GUEST_WRITTEN_MB" | awk '{printf "%.2f", $1/$2}')
            echo "Write amplification: ${WRITE_AMP}x"
            echo ""

            if [ "$(echo "$WRITE_AMP < 2.0" | bc -l)" = "1" ]; then
              echo "✓ Write amplification is healthy (< 2.0x)"
            else
              echo "⚠ Write amplification is elevated (>= 2.0x)"
              echo "  This may indicate inefficient batching or ZFS overhead"
            fi
          fi
          echo ""

          echo "=== BATCH EFFICIENCY ==="
          BATCH_COUNT=$(./mc ls --recursive myminio/zerofs-zfs-test | grep -c "/batch_" || echo "0")
          if [ "$BATCH_COUNT" -gt 0 ] && [ "$S3_USAGE_MB" -gt 0 ]; then
            AVG_BATCH_SIZE=$((S3_USAGE_MB / BATCH_COUNT))
            echo "Total batches: $BATCH_COUNT"
            echo "Average batch size: ${AVG_BATCH_SIZE}MB"
            echo ""

            # With 100 blocks × 128KB = 12.8MB max batch size
            if [ "$AVG_BATCH_SIZE" -ge 10 ]; then
              echo "✓ Good batch efficiency (avg >= 10MB)"
            else
              echo "⚠ Low batch efficiency (avg < 10MB)"
              echo "  Consider tuning blocks_per_batch"
            fi
          fi

      - name: Cleanup
        if: always()
        run: |
          # Export and destroy ZFS pool
          sudo zpool export testpool || true

          # Disconnect NBD device
          sudo nbd-client -d /dev/nbd0 || true

          # Kill ZeroFS
          pkill -f "zerofs run -c zerofs-ci.toml" || true

          # Stop MinIO
          docker stop minio || true
