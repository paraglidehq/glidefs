name: Multi-Node Coordination Test

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  multi-node-test:
    name: Multi-Node Lease & Migration Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Start MinIO
        run: |
          docker run -d \
            --name minio \
            -p 9000:9000 \
            -e MINIO_ROOT_USER=minioadmin \
            -e MINIO_ROOT_PASSWORD=minioadmin \
            minio/minio server /data

          # Wait for MinIO to be ready
          for i in {1..30}; do
            if curl -f http://localhost:9000/minio/health/live; then
              echo "MinIO is ready"
              break
            fi
            sleep 1
          done

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y nbd-client zfsutils-linux wget jq

      - name: Setup MinIO buckets
        run: |
          wget -q https://dl.min.io/client/mc/release/linux-amd64/mc
          chmod +x mc
          ./mc alias set myminio http://localhost:9000 minioadmin minioadmin
          ./mc mb myminio/multi-node-test || true

      - name: Build GlideFS
        working-directory: glidefs
        run: cargo build --profile ci

      - name: Test 1 - Lease Prevents Concurrent Writers
        working-directory: glidefs
        run: |
          echo "=== TEST: Lease prevents concurrent writers ==="

          mkdir -p /tmp/node-a-cache /tmp/node-b-cache

          # Create configs for two nodes
          cat > /tmp/node-a.toml << 'EOF'
          [cache]
          dir = "/tmp/node-a-cache"
          disk_size_gb = 1.0
          memory_size_gb = 0.5

          [storage]
          url = "s3://multi-node-test/lease-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "shared-volume"
          size_gb = 1.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          cat > /tmp/node-b.toml << 'EOF'
          [cache]
          dir = "/tmp/node-b-cache"
          disk_size_gb = 1.0
          memory_size_gb = 0.5

          [storage]
          url = "s3://multi-node-test/lease-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10810"]

          [[servers.nbd.exports]]
          name = "shared-volume"
          size_gb = 1.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Start Node A (should acquire lease)
          echo "Starting Node A..."
          ./target/ci/glidefs run -c /tmp/node-a.toml &
          NODE_A_PID=$!

          # Wait for Node A to start and acquire lease
          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then
              echo "Node A is ready"
              break
            fi
            sleep 1
          done

          # Start Node B (should fail to start or operate in readonly)
          echo "Starting Node B (should fail to acquire lease)..."
          ./target/ci/glidefs run -c /tmp/node-b.toml 2>&1 &
          NODE_B_PID=$!

          # Give Node B time to try acquiring lease
          sleep 5

          # Node B should either have exited or be running but unable to write
          # Check if Node B's port is listening
          if nc -z 127.0.0.1 10810 2>/dev/null; then
            echo "Node B started - checking if it's readonly..."
            # If Node B started, it should be in readonly mode
          else
            echo "Node B correctly refused to start (lease held by Node A)"
          fi

          # Cleanup
          kill $NODE_A_PID 2>/dev/null || true
          kill $NODE_B_PID 2>/dev/null || true
          wait $NODE_A_PID 2>/dev/null || true
          wait $NODE_B_PID 2>/dev/null || true

          echo "✓ Lease test passed"

      - name: Test 2 - Live Migration Protocol
        working-directory: glidefs
        run: |
          echo "=== TEST: Live migration protocol ==="

          # Clean up any stale NBD devices from previous tests
          for i in 0 1 2 3; do
            sudo nbd-client -d /dev/nbd$i 2>/dev/null || true
          done
          sleep 1

          rm -rf /tmp/source-cache /tmp/dest-cache
          mkdir -p /tmp/source-cache /tmp/dest-cache

          # Create source config
          cat > /tmp/source.toml << 'EOF'
          [cache]
          dir = "/tmp/source-cache"
          disk_size_gb = 2.0
          memory_size_gb = 1.0

          [storage]
          url = "s3://multi-node-test/migration-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "migrate-vol"
          size_gb = 2.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Start source node
          echo "Starting source node..."
          ./target/ci/glidefs run -c /tmp/source.toml &
          SOURCE_PID=$!

          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then break; fi
            sleep 1
          done

          # Connect NBD and create ZFS pool on source
          echo "Creating ZFS pool on source..."
          sudo modprobe nbd
          echo "Connecting NBD device..."
          if ! sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N migrate-vol; then
            echo "NBD connection failed! Checking dmesg..."
            dmesg | tail -30
            exit 1
          fi
          sudo zpool create -f migrationpool /dev/nbd0
          sudo zfs create migrationpool/data

          # Write test data
          echo "Writing test data on source..."
          sudo dd if=/dev/urandom of=/mnt/migrationpool-data.bin bs=1M count=50
          sudo mv /mnt/migrationpool-data.bin /migrationpool/data/
          ORIGINAL_CHECKSUM=$(sudo sha256sum /migrationpool/data/migrationpool-data.bin | cut -d' ' -f1)
          echo "Original checksum: $ORIGINAL_CHECKSUM"

          # Create a snapshot
          echo "Creating snapshot..."
          sudo zfs snapshot migrationpool/data@pre-migration

          # Trigger drain (SIGUSR1)
          echo "Draining dirty blocks to S3..."
          kill -USR1 $SOURCE_PID
          sleep 10

          # Unmount and export pool
          echo "Exporting pool from source..."
          sudo zfs unmount migrationpool/data || true
          sudo zpool export migrationpool

          # Disconnect NBD
          sudo nbd-client -d /dev/nbd0

          # Stop source node and release lease
          echo "Stopping source node..."
          kill -TERM $SOURCE_PID
          wait $SOURCE_PID 2>/dev/null || true

          # Wait for port to be free
          for i in {1..10}; do
            if ! nc -z 127.0.0.1 10809 2>/dev/null; then break; fi
            sleep 1
          done

          # Create destination config (same S3 path, different cache)
          cat > /tmp/dest.toml << 'EOF'
          [cache]
          dir = "/tmp/dest-cache"
          disk_size_gb = 2.0
          memory_size_gb = 1.0

          [storage]
          url = "s3://multi-node-test/migration-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "migrate-vol"
          size_gb = 2.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Start destination node
          echo "Starting destination node..."
          ./target/ci/glidefs run -c /tmp/dest.toml &
          DEST_PID=$!

          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then break; fi
            sleep 1
          done

          # Connect NBD and import pool on destination
          echo "Importing pool on destination..."
          sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N migrate-vol
          sudo zpool import migrationpool

          # Verify data integrity
          echo "Verifying data integrity..."
          MIGRATED_CHECKSUM=$(sudo sha256sum /migrationpool/data/migrationpool-data.bin | cut -d' ' -f1)
          echo "Migrated checksum: $MIGRATED_CHECKSUM"

          if [ "$ORIGINAL_CHECKSUM" = "$MIGRATED_CHECKSUM" ]; then
            echo "✓ Migration test PASSED - checksums match!"
          else
            echo "✗ Migration test FAILED - checksums don't match!"
            exit 1
          fi

          # Verify snapshot survived
          if sudo zfs list -t snapshot migrationpool/data@pre-migration > /dev/null 2>&1; then
            echo "✓ Snapshot survived migration"
          else
            echo "✗ Snapshot lost during migration"
            exit 1
          fi

          # Cleanup
          sudo zpool export migrationpool || true
          sudo nbd-client -d /dev/nbd0 || true
          kill -TERM $DEST_PID 2>/dev/null || true
          wait $DEST_PID 2>/dev/null || true

          echo "✓ Live migration test passed"

      - name: Test 3 - Wake From Different Node
        working-directory: glidefs
        run: |
          echo "=== TEST: Wake from different node (cold cache read) ==="

          rm -rf /tmp/writer-cache /tmp/reader-cache
          mkdir -p /tmp/writer-cache /tmp/reader-cache

          # Writer config
          cat > /tmp/writer.toml << 'EOF'
          [cache]
          dir = "/tmp/writer-cache"
          disk_size_gb = 2.0
          memory_size_gb = 1.0

          [storage]
          url = "s3://multi-node-test/wake-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "wake-vol"
          size_gb = 2.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Start writer
          ./target/ci/glidefs run -c /tmp/writer.toml &
          WRITER_PID=$!

          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then break; fi
            sleep 1
          done

          # Connect and write data
          sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N wake-vol
          sudo zpool create -f wakepool /dev/nbd0

          # Write known pattern
          echo "Writing test pattern..."
          sudo dd if=/dev/urandom of=/wakepool/testfile.bin bs=1M count=100
          ORIGINAL_SUM=$(sudo sha256sum /wakepool/testfile.bin | cut -d' ' -f1)
          echo "Original: $ORIGINAL_SUM"

          # Sync and drain
          sync
          sudo zpool sync wakepool
          kill -USR1 $WRITER_PID
          sleep 10

          # Export and stop writer
          sudo zpool export wakepool
          sudo nbd-client -d /dev/nbd0
          kill -TERM $WRITER_PID
          wait $WRITER_PID 2>/dev/null || true

          for i in {1..10}; do
            if ! nc -z 127.0.0.1 10809 2>/dev/null; then break; fi
            sleep 1
          done

          # Reader config (different cache dir = cold cache)
          cat > /tmp/reader.toml << 'EOF'
          [cache]
          dir = "/tmp/reader-cache"
          disk_size_gb = 2.0
          memory_size_gb = 1.0

          [storage]
          url = "s3://multi-node-test/wake-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "wake-vol"
          size_gb = 2.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Start reader (cold cache)
          echo "Starting reader with cold cache..."
          ./target/ci/glidefs run -c /tmp/reader.toml &
          READER_PID=$!

          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then break; fi
            sleep 1
          done

          # Import pool (will read from S3)
          sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N wake-vol
          sudo zpool import wakepool

          # Verify data
          RECOVERED_SUM=$(sudo sha256sum /wakepool/testfile.bin | cut -d' ' -f1)
          echo "Recovered: $RECOVERED_SUM"

          if [ "$ORIGINAL_SUM" = "$RECOVERED_SUM" ]; then
            echo "✓ Wake-from-different-node PASSED!"
          else
            echo "✗ Wake-from-different-node FAILED!"
            exit 1
          fi

          # Cleanup
          sudo zpool export wakepool || true
          sudo nbd-client -d /dev/nbd0 || true
          kill -TERM $READER_PID 2>/dev/null || true

          echo "✓ Wake from different node test passed"

      - name: Test 4 - Rapid Lease Handoff Under Load
        working-directory: glidefs
        run: |
          echo "=== TEST: Rapid lease handoff without data loss ==="

          rm -rf /tmp/handoff-a /tmp/handoff-b
          mkdir -p /tmp/handoff-a /tmp/handoff-b

          cat > /tmp/handoff-a.toml << 'EOF'
          [cache]
          dir = "/tmp/handoff-a"
          disk_size_gb = 1.0
          memory_size_gb = 0.5

          [storage]
          url = "s3://multi-node-test/handoff-test"
          encryption_password = "test-password-123"

          [servers.nbd]
          addresses = ["127.0.0.1:10809"]

          [[servers.nbd.exports]]
          name = "handoff-vol"
          size_gb = 1.0

          [aws]
          access_key_id = "minioadmin"
          secret_access_key = "minioadmin"
          endpoint = "http://localhost:9000"
          allow_http = "true"
          EOF

          # Track all checksums
          declare -a CHECKSUMS

          # Perform 3 handoffs
          for round in 1 2 3; do
            echo "--- Round $round ---"

            # Start node
            ./target/ci/glidefs run -c /tmp/handoff-a.toml &
            NODE_PID=$!

            for i in {1..30}; do
              if nc -z 127.0.0.1 10809; then break; fi
              sleep 1
            done

            sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N handoff-vol || true

            if [ $round -eq 1 ]; then
              sudo zpool create -f handoffpool /dev/nbd0
            else
              sudo zpool import handoffpool || sudo zpool import -f handoffpool
            fi

            # Write some data
            FILENAME="round${round}.bin"
            sudo dd if=/dev/urandom of=/handoffpool/$FILENAME bs=1M count=10 2>/dev/null
            SUM=$(sudo sha256sum /handoffpool/$FILENAME | cut -d' ' -f1)
            CHECKSUMS+=("$SUM")
            echo "Round $round checksum: $SUM"

            # Drain and handoff
            sync
            sudo zpool sync handoffpool
            kill -USR1 $NODE_PID
            sleep 5

            sudo zpool export handoffpool
            sudo nbd-client -d /dev/nbd0
            kill -TERM $NODE_PID
            wait $NODE_PID 2>/dev/null || true

            for i in {1..10}; do
              if ! nc -z 127.0.0.1 10809 2>/dev/null; then break; fi
              sleep 1
            done
          done

          # Final verification
          echo "--- Final Verification ---"
          ./target/ci/glidefs run -c /tmp/handoff-a.toml &
          FINAL_PID=$!

          for i in {1..30}; do
            if nc -z 127.0.0.1 10809; then break; fi
            sleep 1
          done

          sudo nbd-client 127.0.0.1 10809 /dev/nbd0 -N handoff-vol
          sudo zpool import handoffpool

          ALL_PASSED=true
          for round in 1 2 3; do
            FILENAME="round${round}.bin"
            EXPECTED="${CHECKSUMS[$((round-1))]}"
            ACTUAL=$(sudo sha256sum /handoffpool/$FILENAME | cut -d' ' -f1)

            if [ "$EXPECTED" = "$ACTUAL" ]; then
              echo "✓ Round $round data intact"
            else
              echo "✗ Round $round data corrupted!"
              ALL_PASSED=false
            fi
          done

          # Cleanup
          sudo zpool export handoffpool || true
          sudo nbd-client -d /dev/nbd0 || true
          kill -TERM $FINAL_PID 2>/dev/null || true

          if [ "$ALL_PASSED" = true ]; then
            echo "✓ Rapid handoff test PASSED"
          else
            echo "✗ Rapid handoff test FAILED"
            exit 1
          fi

      - name: Cleanup
        if: always()
        run: |
          # Export any remaining ZFS pools
          for pool in migrationpool wakepool handoffpool testpool; do
            sudo zpool export $pool 2>/dev/null || true
          done

          # Disconnect NBD devices
          for i in 0 1 2 3; do
            sudo nbd-client -d /dev/nbd$i 2>/dev/null || true
          done

          # Kill any remaining GlideFS processes
          pkill -f "glidefs run" || true

          # Stop MinIO
          docker stop minio || true
